<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Rajkumar Vaghashiya</title>
    <link>https://rvaghashiya.github.io/tag/deep-learning/</link>
      <atom:link href="https://rvaghashiya.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Rajkumar Vaghashiya</copyright><lastBuildDate>Thu, 09 Jul 2020 23:07:00 +0000</lastBuildDate>
    <image>
      <url>https://rvaghashiya.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Deep Learning</title>
      <link>https://rvaghashiya.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>Computer Pointer Controller</title>
      <link>https://rvaghashiya.github.io/project/computer-pointer-controller/</link>
      <pubDate>Thu, 09 Jul 2020 23:07:00 +0000</pubDate>
      <guid>https://rvaghashiya.github.io/project/computer-pointer-controller/</guid>
      <description>&lt;h1 id=&#34;computer-pointer-controller&#34;&gt;Computer Pointer Controller&lt;/h1&gt;
&lt;p&gt;Computer Pointer Controller is a smart application which leverages pre-trained OpenVino models wherein it infers the gaze direction of the subject in a video or webcam feed, and moves the mouse pointer accordingly in that direction.&lt;/p&gt;
&lt;img src=&#34;./bin/controller.gif&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;
&lt;h2 id=&#34;aim&#34;&gt;Aim&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To develop a smart application that infers the gaze direction of the subject in a video or webcam feed, and moves the mouse pointer accordingly in that direction&lt;/li&gt;
&lt;li&gt;Leverage image processing and pre-trained OpenVINO-models based inference to deliver real-time results&lt;/li&gt;
&lt;li&gt;Analyze the impact of model precision, type of input video streaming, and the effect of various edge cases and their effect on the results&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-set-up-and-installation&#34;&gt;Project Set Up and Installation&lt;/h2&gt;
&lt;h3 id=&#34;setup-details&#34;&gt;Setup Details&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Details&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Programming Language&lt;/td&gt;
&lt;td&gt;Python 3.&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenVino Toolkit Version&lt;/td&gt;
&lt;td&gt;2020.&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hardware Used&lt;/td&gt;
&lt;td&gt;Intel CPU i7 3rd Gen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Enviroments Used&lt;/td&gt;
&lt;td&gt;Windows WSL, Ubuntu 18.04&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;OpenVino installation guide can be found &lt;a href=&#34;https://docs.openvinotoolkit.org/latest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;directory-structure&#34;&gt;Directory Structure&lt;/h3&gt;
&lt;img src=&#34;./bin/directory_struct.png&#34; width=&#34;460&#34; height=&#34;360&#34;&gt;
&lt;h3 id=&#34;application-workflow-pipeline&#34;&gt;Application Workflow Pipeline&lt;/h3&gt;
&lt;img src=&#34;./bin/pipeline.png&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;
&lt;h2 id=&#34;benchmarks&#34;&gt;Benchmarks&lt;/h2&gt;
&lt;h3 id=&#34;performance-evaluation-on-cpu&#34;&gt;Performance Evaluation on CPU&lt;/h3&gt;
&lt;p&gt;Total frames analyzed: 59&lt;/p&gt;
&lt;p&gt;Face Detection Model takes nearly 287 ms of load time and 38 ms of total inference time.&lt;/p&gt;
&lt;p&gt;Model Load Time&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;FP16&lt;/th&gt;
&lt;th&gt;FP32&lt;/th&gt;
&lt;th&gt;FP16-INT8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Facial Landmarks Detection&lt;/td&gt;
&lt;td&gt;122.802 ms&lt;/td&gt;
&lt;td&gt;205.130 ms&lt;/td&gt;
&lt;td&gt;558.811 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Headpose Estimation&lt;/td&gt;
&lt;td&gt;187.159 ms&lt;/td&gt;
&lt;td&gt;340.448 ms&lt;/td&gt;
&lt;td&gt;375.702 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gaze Estimation&lt;/td&gt;
&lt;td&gt;206.344 ms&lt;/td&gt;
&lt;td&gt;361.897 ms&lt;/td&gt;
&lt;td&gt;455.502 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Total Load Time&lt;/td&gt;
&lt;td&gt;0.82 s&lt;/td&gt;
&lt;td&gt;1.188 s&lt;/td&gt;
&lt;td&gt;1.67 s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Model Inference Time&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;FP16&lt;/th&gt;
&lt;th&gt;FP32&lt;/th&gt;
&lt;th&gt;FP16-INT8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Facial Landmarks Detection&lt;/td&gt;
&lt;td&gt;1.0 ms&lt;/td&gt;
&lt;td&gt;1.0 ms&lt;/td&gt;
&lt;td&gt;1.1 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Headpose Estimation&lt;/td&gt;
&lt;td&gt;2.7 ms&lt;/td&gt;
&lt;td&gt;2.7 ms&lt;/td&gt;
&lt;td&gt;2.6 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gaze Estimation&lt;/td&gt;
&lt;td&gt;3.4 ms&lt;/td&gt;
&lt;td&gt;3.3 ms&lt;/td&gt;
&lt;td&gt;2.5 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Total Inference Time&lt;/td&gt;
&lt;td&gt;11.1 s&lt;/td&gt;
&lt;td&gt;11.4 s&lt;/td&gt;
&lt;td&gt;11.7 s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FPS&lt;/td&gt;
&lt;td&gt;5.315&lt;/td&gt;
&lt;td&gt;5.363&lt;/td&gt;
&lt;td&gt;5.042&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The inference times with respect to the models depict the average inference time per frame.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The results have been benchmarked on async inference&lt;/li&gt;
&lt;li&gt;Model Load time for FP16 is the least, followed by FP32, followed by INT8&lt;/li&gt;
&lt;li&gt;Average inference time is greatest for INT8, followed by FP32, followed by FP16, which may be attributed to the trade-off between greater information involved in the computation of models having relatively higher precision and the hardware architecture of the system under use.&lt;/li&gt;
&lt;li&gt;A decrease in the model precision leads to loss of the model accuracy at the cost of increasing the inference speed.&lt;/li&gt;
&lt;li&gt;Frames are processed relatively faster in FP32, followed by FP16 and then INT8.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;edge-cases&#34;&gt;Edge Cases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In case a person is not detected in the frame, the incident is logged and processing moves on to the next frame in the queue.&lt;/li&gt;
&lt;li&gt;In case of multiple persons in the frame, only the face of the first person to be detected will be used in the further processing.&lt;/li&gt;
&lt;li&gt;In case the new mouse corrdinates from gaze estimation cannot be accomodated on the current screen resolution, the event is logged and processing moves onto next frame in the sequence.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/raj-98/Computer-Pointer-Controller&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Git: Computer Pointer Controller&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>People Counter App</title>
      <link>https://rvaghashiya.github.io/project/people-counter-app/</link>
      <pubDate>Thu, 25 Jun 2020 22:50:00 +0000</pubDate>
      <guid>https://rvaghashiya.github.io/project/people-counter-app/</guid>
      <description>&lt;h1 id=&#34;deploy-a-people-counter-app-at-the-edge&#34;&gt;Deploy a People Counter App at the Edge&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Details&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Programming Language:&lt;/td&gt;
&lt;td&gt;Python 3.5 or 3.6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;./people-counter-image.png&#34; alt=&#34;people-counter-python&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-it-does&#34;&gt;What it Does&lt;/h2&gt;
&lt;p&gt;The people counter application will demonstrate how to create a smart video IoT solution using Intel® hardware and software tools. The app will detect people in a designated area, providing the number of people in the frame, average duration of people in frame, and total count.&lt;/p&gt;
&lt;h2 id=&#34;aim&#34;&gt;Aim&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Create an app that will help maintain social distancing at public places likes grocery stores and shopping malls, to curb the spread of Covid-19 virus&lt;/li&gt;
&lt;li&gt;The app will detect people in a designated area, providing the number of people in the frame, average duration of people in frame, and total count.&lt;/li&gt;
&lt;li&gt;The people counter application is a smart video IoT solution based on Intel® hardware and software tools.&lt;/li&gt;
&lt;li&gt;Utilizes deep learning, image processing, and OpenVINO-based inferencing at the edge.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-it-works&#34;&gt;How it Works&lt;/h2&gt;
&lt;p&gt;The counter will use the Inference Engine included in the Intel® Distribution of OpenVINO™ Toolkit. The model used should be able to identify people in a video frame. The app should count the number of people in the current frame, the duration that a person is in the frame (time elapsed between entering and exiting a frame) and the total count of people. It then sends the data to a local web server using the Paho MQTT Python package.&lt;/p&gt;
&lt;p&gt;You will choose a model to use and convert it with the Model Optimizer.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./arch_diagram.png&#34; alt=&#34;architectural diagram&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;h3 id=&#34;hardware&#34;&gt;Hardware&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;6th to 10th generation Intel® Core™ processor with Iris® Pro graphics or Intel® HD Graphics.&lt;/li&gt;
&lt;li&gt;OR use of Intel® Neural Compute Stick 2 (NCS2)&lt;/li&gt;
&lt;li&gt;OR Udacity classroom workspace for the related course&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;software&#34;&gt;Software&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Intel® Distribution of OpenVINO™ toolkit 2019 R3 release&lt;/li&gt;
&lt;li&gt;Node v6.17.1&lt;/li&gt;
&lt;li&gt;Npm v3.10.10&lt;/li&gt;
&lt;li&gt;CMake&lt;/li&gt;
&lt;li&gt;MQTT Mosca server&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/raj-98/People-Counter-App&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Git: People Counter App&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/udacity/nd131-openvino-fundamentals-project-starter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reference Repo&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intelligent Digital Inline Holographic Micrograph (DIHM) Cell-Enhancement and Characterization</title>
      <link>https://rvaghashiya.github.io/post/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/index1/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://rvaghashiya.github.io/post/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/index1/</guid>
      <description>&lt;h2 id=&#34;intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization&#34;&gt;Intelligent Digital Inline Holographic Micrograph (DIHM) Cell-Enhancement and Characterization&lt;/h2&gt;
&lt;h2 id=&#34;project-intro&#34;&gt;Project Intro&lt;/h2&gt;
&lt;p&gt;Developing a novel and efficient neural model for detecting and classifying cells such as RBC, WBC, and cancer cells, etc.(cellular pathology) from DIH(Digital Inline Holographic) microscopy images&lt;/p&gt;
&lt;p&gt;Implementation on a resource-constrained device to develop a cheap, reliable and portable point-of-care testing facility for diagnosis of pathological diseases, especially for usage in rural areas&lt;/p&gt;
&lt;h2 id=&#34;highlights&#34;&gt;Highlights&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Segment cell-lines in DIH micrograph; performs signal enhancement using CNN-based autoencoder, followed by the cell-line characterization&lt;/li&gt;
&lt;li&gt;ROC-AUC: &amp;gt;0.98 for RBC, WBC, and microbeads; &amp;gt;0.88 for cancer cells HepG2 and MCF7&lt;/li&gt;
&lt;li&gt;Easy accommodation of newer cell-lines. Python, TensorFlow, OpenCV&lt;/li&gt;
&lt;li&gt;Preliminary work &lt;a href=&#34;https://ieeexplore.ieee.org/document/9374330&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;published&lt;/a&gt; at the 8th IEEE ICHI&lt;/li&gt;
&lt;/ul&gt;














&lt;figure  id=&#34;figure-breast-cancer-disease-statistics&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;img1.png&#34; alt=&#34;Breast Cancer Disease Statistics&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Breast Cancer Disease Statistics
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-limitations-of-traditional-optic-microscopy&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;img2.png&#34; alt=&#34;Limitations of Traditional Optic Microscopy&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Limitations of Traditional Optic Microscopy
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;aim&#34;&gt;Aim&lt;/h2&gt;
&lt;p&gt;The aim of the research is to create a&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cheap,&lt;/li&gt;
&lt;li&gt;Reliable,&lt;/li&gt;
&lt;li&gt;Adaptable,&lt;/li&gt;
&lt;li&gt;Portable, and&lt;/li&gt;
&lt;li&gt;Intelligent
real-time point-of-care testing facility that could be used in resource-constrained environments, especially such as those in a rural setting.&lt;/li&gt;
&lt;/ul&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;img3.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;














&lt;figure  id=&#34;figure-cell-lines-used-in-the-project&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;cells.png&#34; alt=&#34;Cell-lines used in the project&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Cell-lines used in the project
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;current-progress&#34;&gt;Current Progress&lt;/h2&gt;














&lt;figure  id=&#34;figure-cell-line-eda&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;img4.png&#34; alt=&#34;Cell-line EDA&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Cell-line EDA
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The colored cell-lines is the result of EDA on the statistical properties and pixel intensity in the images&lt;/p&gt;
&lt;h2 id=&#34;segmentation-results&#34;&gt;Segmentation Results&lt;/h2&gt;














&lt;figure  id=&#34;figure-the-segmentation-involves-bit-plane-splicing-adaptive-thresholding-and-contour-approximation-to-crop-the-cell-lines&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;segmented_cells.png&#34; alt=&#34;The segmentation involves bit-plane splicing, adaptive thresholding, and contour approximation to crop the cell-lines&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The segmentation involves bit-plane splicing, adaptive thresholding, and contour approximation to crop the cell-lines
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;cnn-model-performance&#34;&gt;CNN Model Performance&lt;/h2&gt;














&lt;figure  id=&#34;figure-confusion-matrix&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;confmatrix_with_acc_prevv.png&#34; alt=&#34;Confusion Matrix&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Confusion Matrix
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The recognition performance of the CNN model on the augmented dataset&lt;/p&gt;
&lt;h2 id=&#34;final-cell-counts-for-the-input-micrograph&#34;&gt;Final Cell Counts for the Input Micrograph&lt;/h2&gt;














&lt;figure  id=&#34;figure-the-final-result-in-form-of-cell-counts-for-the-roi-region-of-interest-window-selected-in-the-dih-micrograph&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;final_count.png&#34; alt=&#34;The final result in form of cell-counts for the ROI (region of interest) window selected in the DIH micrograph&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The final result in form of cell-counts for the ROI (region of interest) window selected in the DIH micrograph
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Intelligent Digital Inline Holographic Micrograph (DIHM) Cell-Enhancement and Characterization</title>
      <link>https://rvaghashiya.github.io/project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/</link>
      <pubDate>Mon, 07 Jan 2019 21:20:00 +0000</pubDate>
      <guid>https://rvaghashiya.github.io/project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/</guid>
      <description>&lt;h2 id=&#34;project-intro&#34;&gt;Project Intro&lt;/h2&gt;
&lt;p&gt;Developing a novel and efficient neural model for detecting and classifying cells such as RBC, WBC, and cancer cells, etc. (cellular pathology) from DIH (Digital Inline Holographic) microscopy images&lt;/p&gt;
&lt;p&gt;Implementation on a resource-constrained device to develop a cheap, reliable and portable point-of-care testing facility for diagnosis of pathological diseases, especially for usage in rural areas&lt;/p&gt;
&lt;h2 id=&#34;highlights&#34;&gt;Highlights&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Segment cell-lines in DIH micrograph; performs signal enhancement using CNN-based autoencoder, followed by the cell-line characterization&lt;/li&gt;
&lt;li&gt;ROC-AUC: &amp;gt;0.98 for RBC, WBC, and microbeads; &amp;gt;0.88 for cancer cells HepG2 and MCF7&lt;/li&gt;
&lt;li&gt;Easy accommodation of newer cell-lines. Python, TensorFlow, OpenCV&lt;/li&gt;
&lt;li&gt;Preliminary work &lt;a href=&#34;https://ieeexplore.ieee.org/document/9374330&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;published&lt;/a&gt; at the 8th IEEE ICHI&lt;/li&gt;
&lt;/ul&gt;














&lt;figure  id=&#34;figure-breast-cancer-disease-statistics&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Breast Cancer Disease Statistics&#34; srcset=&#34;
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img1_hu8ac06d34c690e9869143e5654758e9be_185519_475645bba04960e1e4ed3194f41c0583.png 400w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img1_hu8ac06d34c690e9869143e5654758e9be_185519_03ab957a36f7fe6afd072645b45cced3.png 760w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img1_hu8ac06d34c690e9869143e5654758e9be_185519_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://rvaghashiya.github.io/project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img1_hu8ac06d34c690e9869143e5654758e9be_185519_475645bba04960e1e4ed3194f41c0583.png&#34;
               width=&#34;666&#34;
               height=&#34;292&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Breast Cancer Disease Statistics
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-limitations-of-traditional-optic-microscopy&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Limitations of Traditional Optic Microscopy&#34; srcset=&#34;
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img2_hub55ec7b5fd66b1f4d1af27ec086b55cb_100527_4ad8d8c8f47d358ce0a83eae8c2f7638.png 400w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img2_hub55ec7b5fd66b1f4d1af27ec086b55cb_100527_10cfcac97155a0ac411dc43a8d69a609.png 760w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img2_hub55ec7b5fd66b1f4d1af27ec086b55cb_100527_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://rvaghashiya.github.io/project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img2_hub55ec7b5fd66b1f4d1af27ec086b55cb_100527_4ad8d8c8f47d358ce0a83eae8c2f7638.png&#34;
               width=&#34;702&#34;
               height=&#34;406&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Limitations of Traditional Optic Microscopy
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;aim&#34;&gt;Aim&lt;/h2&gt;
&lt;p&gt;The aim of the research is to create a&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cheap,&lt;/li&gt;
&lt;li&gt;Reliable,&lt;/li&gt;
&lt;li&gt;Adaptable,&lt;/li&gt;
&lt;li&gt;Portable, and&lt;/li&gt;
&lt;li&gt;Intelligent
real-time point-of-care testing facility that could be used in resource-constrained environments, especially such as those in a rural setting.&lt;/li&gt;
&lt;/ul&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img3_hu3437bf12853f01934bb7d5a92b93994c_57883_d4bd2fc0283ce1cce6c3a84d7eb1a538.png 400w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img3_hu3437bf12853f01934bb7d5a92b93994c_57883_f4366c281399035b0fac8fd7ceeccce4.png 760w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img3_hu3437bf12853f01934bb7d5a92b93994c_57883_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://rvaghashiya.github.io/project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img3_hu3437bf12853f01934bb7d5a92b93994c_57883_d4bd2fc0283ce1cce6c3a84d7eb1a538.png&#34;
               width=&#34;728&#34;
               height=&#34;412&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;














&lt;figure  id=&#34;figure-cell-lines-used-in-the-project&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Cell-lines used in the project&#34; srcset=&#34;
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/cells_hudd1927cb78bec53e9676f04b0e2d8b7b_109686_45d1dd88b1f2f935b77edddc1f4bda73.png 400w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/cells_hudd1927cb78bec53e9676f04b0e2d8b7b_109686_68013224ae7cdce72d93e6a4edf8a6ca.png 760w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/cells_hudd1927cb78bec53e9676f04b0e2d8b7b_109686_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://rvaghashiya.github.io/project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/cells_hudd1927cb78bec53e9676f04b0e2d8b7b_109686_45d1dd88b1f2f935b77edddc1f4bda73.png&#34;
               width=&#34;602&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Cell-lines used in the project
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;current-progress&#34;&gt;Current Progress&lt;/h2&gt;














&lt;figure  id=&#34;figure-cell-line-eda&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Cell-line EDA&#34; srcset=&#34;
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img4_hua953d76968a823f2abe1a0204f1da5bb_158154_ecb6efbc9ced2d0404ffb584bbb606b3.png 400w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img4_hua953d76968a823f2abe1a0204f1da5bb_158154_8b98c151d1de0d173a3c10a7e8930067.png 760w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img4_hua953d76968a823f2abe1a0204f1da5bb_158154_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://rvaghashiya.github.io/project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/img4_hua953d76968a823f2abe1a0204f1da5bb_158154_ecb6efbc9ced2d0404ffb584bbb606b3.png&#34;
               width=&#34;712&#34;
               height=&#34;370&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Cell-line EDA
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The colored cell-lines is the result of EDA on the statistical properties and pixel intensity in the images&lt;/p&gt;
&lt;h2 id=&#34;segmentation-results&#34;&gt;Segmentation Results&lt;/h2&gt;














&lt;figure  id=&#34;figure-the-segmentation-involves-bit-plane-splicing-adaptive-thresholding-and-contour-approximation-to-crop-the-cell-lines&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;The segmentation involves bit-plane splicing, adaptive thresholding, and contour approximation to crop the cell-lines&#34; srcset=&#34;
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/segmented_cells_hub292d90b0ae535b51923518faf2fcee3_120666_1dd7f66681a85ae24ac7a19c62f6ede9.png 400w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/segmented_cells_hub292d90b0ae535b51923518faf2fcee3_120666_258093a48aae7f4892943cddae8dc6a6.png 760w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/segmented_cells_hub292d90b0ae535b51923518faf2fcee3_120666_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://rvaghashiya.github.io/project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/segmented_cells_hub292d90b0ae535b51923518faf2fcee3_120666_1dd7f66681a85ae24ac7a19c62f6ede9.png&#34;
               width=&#34;738&#34;
               height=&#34;334&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The segmentation involves bit-plane splicing, adaptive thresholding, and contour approximation to crop the cell-lines
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;cnn-model-performance&#34;&gt;CNN Model Performance&lt;/h2&gt;














&lt;figure  id=&#34;figure-confusion-matrix&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Confusion Matrix&#34; srcset=&#34;
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/confmatrix_with_acc_prevv_hu57938ce728a6ebcdb8a739e041244294_158370_b4a3c743645612dee76e5d578afc06da.png 400w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/confmatrix_with_acc_prevv_hu57938ce728a6ebcdb8a739e041244294_158370_738d36a2eebe60d312aa813ab7c46d1a.png 760w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/confmatrix_with_acc_prevv_hu57938ce728a6ebcdb8a739e041244294_158370_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://rvaghashiya.github.io/project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/confmatrix_with_acc_prevv_hu57938ce728a6ebcdb8a739e041244294_158370_b4a3c743645612dee76e5d578afc06da.png&#34;
               width=&#34;760&#34;
               height=&#34;634&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Confusion Matrix
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The recognition performance of the CNN model on the augmented dataset&lt;/p&gt;
&lt;h2 id=&#34;final-cell-counts-for-the-input-micrograph&#34;&gt;Final Cell Counts for the Input Micrograph&lt;/h2&gt;














&lt;figure  id=&#34;figure-the-final-result-in-form-of-cell-counts-for-the-roi-region-of-interest-window-selected-in-the-dih-micrograph&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;The final result in form of cell-counts for the ROI (region of interest) window selected in the DIH micrograph&#34; srcset=&#34;
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/final_count_hu4532d829720968e51c8a947e44bbd19c_17679_b5322d216eca735541c8dbd138829b7f.png 400w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/final_count_hu4532d829720968e51c8a947e44bbd19c_17679_1b990b8007c4095791e64b6e3d8df7e6.png 760w,
               /project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/final_count_hu4532d829720968e51c8a947e44bbd19c_17679_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://rvaghashiya.github.io/project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/final_count_hu4532d829720968e51c8a947e44bbd19c_17679_b5322d216eca735541c8dbd138829b7f.png&#34;
               width=&#34;630&#34;
               height=&#34;470&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The final result in form of cell-counts for the ROI (region of interest) window selected in the DIH micrograph
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
  </channel>
</rss>
