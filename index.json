[{"authors":null,"categories":null,"content":"ðŸ‘‹ Hi there, Iâ€™m a MSc Computer Science student with 3+ years of experience in Deep Learning and Computer Vision. I specialize in developing robust AI systems that bridge cutting-edge research with real-world applications.\nIâ€™m passionate about interdisciplinary AI applications to solve real-world challenges and translate AI insights into actionable decisions for optimizing critical workflows.\nðŸ˜€ Iâ€™m always open to collaborations on AI, Computer Vision, and Applied Machine Learning projects.\nCheck my CV.\n","date":1645986034,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1645986034,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"ðŸ‘‹ Hi there, Iâ€™m a MSc Computer Science student with 3+ years of experience in Deep Learning and Computer Vision. I specialize in developing robust AI systems that bridge cutting-edge research with real-world applications.","tags":null,"title":"Rajkumar Vaghashiya","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemyâ€™s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://rvaghashiya.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Rajkumar Vaghashiya","Sanghoon Shin","Varun Chauhan","Kaushal Kapadiya","Smit Sanghavi","Sungkyu Seo","Mohendra Roy"],"categories":null,"content":"","date":1645986034,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645986034,"objectID":"c8a8104d518b25397dd4296c47c66dd6","permalink":"https://rvaghashiya.github.io/publication/machine-learning-based-lens---free-shadow-imaging-technique-for-field---portable-cytometry/","publishdate":"2022-02-27T18:20:34.903Z","relpermalink":"/publication/machine-learning-based-lens---free-shadow-imaging-technique-for-field---portable-cytometry/","section":"publication","summary":"","tags":["Machine Learning","Shadow Imaging","Field-Portable Cytometry","Cell-line denoising","Cell-line classification","Point of Care Diagnosis"],"title":"Machine Learning Based Lens-Free Shadow Imaging Technique for Field-Portable Cytometry","type":"publication"},{"authors":["Rajkumar Vaghashiya","Anagha Zachariah","Ekaterina Nepovinnykh","Phuc Nguyen","Md.Sadek Hossain Asif","Amrit Kumar Jethi","Jonas Adler (Senior Research Scientist, DeepMind)"],"categories":null,"content":"MediSinGAN The aim is to use an unconditional generative model, SinGAN, to augment medical image datasets using a single natural image.\nCurrent Progress Implemented SinGAN architecture in JAX for the generation of realistic synthetic medical imaging data using a single training image and achieved a 20% reduction in training time Evaluated the model applicability in MRI cross-modality image-to-image translation, Synthetic brain tumor generation, and Medical image segmentation (Histopathology) ","date":1625180820,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"af5166189b5ee05beae7682f705cfaa4","permalink":"https://rvaghashiya.github.io/project/medisingan/","publishdate":"2021-07-01T23:07:00Z","relpermalink":"/project/medisingan/","section":"project","summary":"An unconditional generative model to augment medical image datasets using a single natural image.","tags":["Generative Networks","Medical Image Analysis","Computer Vision","Deep Learning","Research"],"title":"MediSinGAN","type":"project"},{"authors":["Rajkumar Vaghashiya","Kaushal Kapadiya","Ishita Nandwani","Riya Thakore","Dongmin Seo","Sungkyu Seo","Mohendra Roy"],"categories":null,"content":"","date":1606760434,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606760434,"objectID":"5ceb4fc2ba64be673f8720a5bc4d0380","permalink":"https://rvaghashiya.github.io/publication/an-optimized-neural-network-architecture-for-auto-characterization-of-biological-cells-in-digital-inline-holography-micrographs/","publishdate":"2020-11-30T18:20:34.903Z","relpermalink":"/publication/an-optimized-neural-network-architecture-for-auto-characterization-of-biological-cells-in-digital-inline-holography-micrographs/","section":"publication","summary":"","tags":["AI","Digital Inline Holographic Microscopy","Cell-line classification","Point of Care Diagnosis"],"title":"An Optimized Neural Network Architecture for Auto Characterization of Biological Cells in Digital Inline Holography Micrographs","type":"publication"},{"authors":["Rajkumar Vaghashiya","Dr. Abhijit Roy (Assistant Professor, IISc)","Dr. Mohendra Roy (Assistant Professor, PDPU)"],"categories":null,"content":"Project Goals Cost-effective and portable ELISA reader with AI-aided analyzer and adaptive calibration Image processing based qualitative and quantitative real-time analysis Current Progress Completed precise micro-plate well localization Well localization utilizes bit plane slicing, masking and contouring, and shape appropriation Extracts basic color information, measuring light intensity in RBG colorspace, for each well Provides normalized measure of intenstity Note Uses standard well-reference: A-H for rows, 1 to 12 for columns â€˜00â€™ for base/reference well Sample: Segmented Microplate-wells (Pic shows a zoomed section of the microplate) Sample: Segmented Microplate-wells (Pic shows a zoomed section of the microplate) Locations of Segmented Wells Locations of Segmented Wells A cropped Well filled with a solution A cropped empty Well Well boundary colored has been colored for reference in the above figures.\nNote: Full work till date cannot be disclosed\n","date":1598993520,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"911f932732ac685226653e6826ce24e3","permalink":"https://rvaghashiya.github.io/project/smart-portable-and-cost-effective-elisa-reader/","publishdate":"2020-09-01T20:52:00Z","relpermalink":"/project/smart-portable-and-cost-effective-elisa-reader/","section":"project","summary":"A microplate well segmentation pipeline for real-time colorimetric analysis of microplate wells.","tags":["Machine Learning","Image Segmentation","Colorimetry Analysis","Research","Academic","ELISA"],"title":"Smart, Portable, and Cost-effective ELISA Reader","type":"project"},{"authors":["Rajkumar Vaghashiya","Dr. Abhijit Roy (Assistant Professor, IISc)","Dr. Mohendra Roy (Assistant Professor, PDPU)"],"categories":["Demo"],"content":"Project Goals Cost-effective and portable ELISA reader with AI-aided diagnosis for adaptive calibration Image processing based qualitative and quantitative real-time analysis for improved sensitivity and specificity Current Progress Completed precise micro-plate well localization Well localization utilizes bit plane slicing, masking and contouring, and shape appropriation Extracts basic color information, measuring light intensity in RBG colorspace, for each well Provides normalized measure of intenstity Note Uses standard well-reference: A-H for rows, 1 to 12 for columns â€˜00â€™ for base/reference well Sample: Segmented Microplate-wells (Pic shows a zoomed section of the microplate) Sample: Segmented Microplate-wells (Pic shows a zoomed section of the microplate) Locations of Segmented Wells Locations of Segmented Wells A cropped Well filled with a solution A cropped empty Well Well boundary colored has been colored for reference in the above figures.\nNote: Full work till date cannot be disclosed\n","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"cf20a759bc9fa417eb38948edf4d640f","permalink":"https://rvaghashiya.github.io/post/smart-portable-and-cost-effective-elisa-reader/index1/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/post/smart-portable-and-cost-effective-elisa-reader/index1/","section":"post","summary":"Project Goals Cost-effective and portable ELISA reader with AI-aided diagnosis for adaptive calibration Image processing based qualitative and quantitative real-time analysis for improved sensitivity and specificity Current Progress Completed precise micro-plate well localization Well localization utilizes bit plane slicing, masking and contouring, and shape appropriation Extracts basic color information, measuring light intensity in RBG colorspace, for each well Provides normalized measure of intenstity Note Uses standard well-reference: A-H for rows, 1 to 12 for columns â€˜00â€™ for base/reference well Sample: Segmented Microplate-wells (Pic shows a zoomed section of the microplate) Sample: Segmented Microplate-wells (Pic shows a zoomed section of the microplate) Locations of Segmented Wells Locations of Segmented Wells A cropped Well filled with a solution A cropped empty Well Well boundary colored has been colored for reference in the above figures.","tags":["Academic","Research","ELISA"],"title":"Smart, Portable, and Cost-effective ELISA Reader","type":"post"},{"authors":["Rajkumar Vaghashiya","Dr. Mohendra Roy (Assistant Professor, PDPU)"],"categories":["Demo"],"content":"DIH-Cell-Augmentor A setup for augmentation of biological cells captured via Digital Inline Holography\nAim A script to augment the cell-lines obtained via DIHM. It generates rotated copies of the input cell sample, while maintaining the same image dimensions and near-similar sample background, using OpenCV. The augmented samples preserve the spatial features as well as the statistical distribution present in the input sample. Method Load dataset, having images grouped under labels Augment cell sample by rotation at specific angle Store augmented images in a csv Augmentation Procedure Create a mask to extract the cell background Negate the above mask to extract the cell Rotate the image by specified degree, achieved via getRotationMatrix2D and warpAffine functions in cv2 Extract the rotated cell signature using the negated mask Mask it with the extracted cell background to create an augmented image, with same shape as original image Note: The code is applicable only for augmentation of objects/cells lying within the maximum fitting circle which can be fit into the image\nPre-requisites: cv2 glob, or alternatively use os numpy Demo: Input Image: Augmented Images: Augmented Images on rotation by 0, 10, 20, 30,â€¦..,350 degrees :\nNote: Black dot has been added for easier vizualization\nGit: DIH Cell Augmentor\n","date":1596323340,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596323340,"objectID":"6917cbfab6c0d5455a42330b6b4d7f4d","permalink":"https://rvaghashiya.github.io/project/dih-cell-augmentor/","publishdate":"2020-08-01T23:09:00Z","relpermalink":"/project/dih-cell-augmentor/","section":"project","summary":"Cell-line image augmentation for biological cells captured via Digital Inline Holographic Microscopy.","tags":["DIH Microscopy","Image Data Augmentation","Other"],"title":"DIH-Cell-Augmentor","type":"project"},{"authors":["Rajkumar Vaghashiya"],"categories":["Demo"],"content":"Computer Pointer Controller Computer Pointer Controller is a smart application which leverages pre-trained OpenVino models wherein it infers the gaze direction of the subject in a video or webcam feed, and moves the mouse pointer accordingly in that direction.\nAim To develop a smart application that infers the gaze direction of the subject in a video or webcam feed, and moves the mouse pointer accordingly in that direction Leverage image processing and pre-trained OpenVINO-models based inference to deliver real-time results Analyze the impact of model precision, type of input video streaming, and the effect of various edge cases and their effect on the results Project Set Up and Installation Setup Details Details Programming Language Python 3.6 OpenVino Toolkit Version 2020.3 Hardware Used Intel CPU i7 3rd Gen Enviroments Used Windows WSL, Ubuntu 18.04 OpenVino installation guide can be found here.\nDirectory Structure Application Workflow Pipeline Benchmarks Performance Evaluation on CPU Total frames analyzed: 59\nFace Detection Model takes nearly 287 ms of load time and 38 ms of total inference time.\nModel Load Time\nModel FP16 FP32 FP16-INT8 Facial Landmarks Detection 122.802 ms 205.130 ms 558.811 ms Headpose Estimation 187.159 ms 340.448 ms 375.702 ms Gaze Estimation 206.344 ms 361.897 ms 455.502 ms Total Load Time 0.82 s 1.188 s 1.67 s Model Inference Time\nModel FP16 FP32 FP16-INT8 Facial Landmarks Detection 1.0 ms 1.0 ms 1.1 ms Headpose Estimation 2.7 ms 2.7 ms 2.6 ms Gaze Estimation 3.4 ms 3.3 ms 2.5 ms Total Inference Time 11.1 s 11.4 s 11.7 s FPS 5.315 5.363 5.042 The inference times with respect to the models depict the average inference time per frame.\nResults The results have been benchmarked on async inference Model Load time for FP16 is the least, followed by FP32, followed by INT8 Average inference time is greatest for INT8, followed by FP32, followed by FP16, which may be attributed to the trade-off between greater information involved in the computation of models having relatively higher precision and the hardware architecture of the system under use. A decrease in the model precision leads to loss of the model accuracy at the cost of increasing the inference speed. Frames are processed relatively faster in FP32, followed by FP16 and then INT8. Edge Cases In case a person is not detected in the frame, the incident is logged and processing moves on to the next frame in the queue. In case of multiple persons in the frame, only the face of the first person to be detected will be used in the further processing. In case the new mouse corrdinates from gaze estimation cannot be accomodated on the current screen resolution, the event is logged and processing moves onto next frame in the sequence. Git: Computer Pointer Controller\n","date":1594336020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594336020,"objectID":"91e01c9824ba731441e0863409abad0d","permalink":"https://rvaghashiya.github.io/project/computer-pointer-controller/","publishdate":"2020-07-09T23:07:00Z","relpermalink":"/project/computer-pointer-controller/","section":"project","summary":"A smart application to control the cursor based on the eye-gaze direction of a subject in a video-feed using OpenVINO toolkit. Leverages image processing and pre-trained OpenVINO-models to deliver real-time results.","tags":["OpenVINO","Image Analytics","Eye Gaze Tracking","Deep Learning","Edge AI"],"title":"Computer Pointer Controller","type":"project"},{"authors":["Rajkumar Vaghashiya"],"categories":["Demo"],"content":"Deploy a People Counter App at the Edge Details Programming Language: Python 3.5 or 3.6 What it Does The people counter application will demonstrate how to create a smart video IoT solution using IntelÂ® hardware and software tools. The app will detect people in a designated area, providing the number of people in the frame, average duration of people in frame, and total count.\nAim Create an app that will help maintain social distancing at public places likes grocery stores and shopping malls, to curb the spread of Covid-19 virus The app will detect people in a designated area, providing the number of people in the frame, average duration of people in frame, and total count. The people counter application is a smart video IoT solution based on IntelÂ® hardware and software tools. Utilizes deep learning, image processing, and OpenVINO-based inferencing at the edge. How it Works The counter will use the Inference Engine included in the IntelÂ® Distribution of OpenVINOâ„¢ Toolkit. The model used should be able to identify people in a video frame. The app should count the number of people in the current frame, the duration that a person is in the frame (time elapsed between entering and exiting a frame) and the total count of people. It then sends the data to a local web server using the Paho MQTT Python package.\nYou will choose a model to use and convert it with the Model Optimizer.\nRequirements Hardware 6th to 10th generation IntelÂ® Coreâ„¢ processor with IrisÂ® Pro graphics or IntelÂ® HD Graphics. OR use of IntelÂ® Neural Compute Stick 2 (NCS2) OR Udacity classroom workspace for the related course Software IntelÂ® Distribution of OpenVINOâ„¢ toolkit 2019 R3 release Node v6.17.1 Npm v3.10.10 CMake MQTT Mosca server Setup Git: People Counter App\nReference Repo\n","date":1593125400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593125400,"objectID":"af5eb49f4111d92428aa0e263dfd8a9e","permalink":"https://rvaghashiya.github.io/project/people-counter-app/","publishdate":"2020-06-25T22:50:00Z","relpermalink":"/project/people-counter-app/","section":"project","summary":"OpenVINO-based Person detection and monitoring app to detect people in a designated area and deliver statistics based on the same.","tags":["OpenVINO","Image Analytics","People Counter","Deep Learning","Edge AI"],"title":"People Counter App","type":"project"},{"authors":["Riya Thakore","Rajkumar Vaghashiya","Chintan Patel","Nishant Doshi"],"categories":null,"content":"","date":1566239782,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566239782,"objectID":"bcc35d60402e92b36a5f958927922d4e","permalink":"https://rvaghashiya.github.io/publication/blockchain-based-iot-a-survey/","publishdate":"2019-08-19T18:36:22.694Z","relpermalink":"/publication/blockchain-based-iot-a-survey/","section":"publication","summary":"","tags":["Internet of Things (IoT)","Blockchain","Applications"],"title":"Blockchain - based IoT: A Survey","type":"publication"},{"authors":["Rajkumar Vaghashiya","Dr. Mohendra Roy [(Assistant Professor, PDPU)](https://sites.google.com/view/mohendraroylab/home)"],"categories":["Demo"],"content":"Intelligent Digital Inline Holographic Micrograph (DIHM) Cell-Enhancement and Characterization Project Intro Developing a novel and efficient neural model for detecting and classifying cells such as RBC, WBC, and cancer cells, etc.(cellular pathology) from DIH(Digital Inline Holographic) microscopy images\nImplementation on a resource-constrained device to develop a cheap, reliable and portable point-of-care testing facility for diagnosis of pathological diseases, especially for usage in rural areas\nHighlights Segment cell-lines in DIH micrograph; performs signal enhancement using CNN-based autoencoder, followed by the cell-line characterization ROC-AUC: \u0026gt;0.98 for RBC, WBC, and microbeads; \u0026gt;0.88 for cancer cells HepG2 and MCF7 Easy accommodation of newer cell-lines. Python, TensorFlow, OpenCV Preliminary work published at the 8th IEEE ICHI Breast Cancer Disease Statistics Limitations of Traditional Optic Microscopy Aim The aim of the research is to create a\nCheap, Reliable, Adaptable, Portable, and Intelligent real-time point-of-care testing facility that could be used in resource-constrained environments, especially such as those in a rural setting. Dataset Cell-lines used in the project Current Progress Cell-line EDA The colored cell-lines is the result of EDA on the statistical properties and pixel intensity in the images\nSegmentation Results The segmentation involves bit-plane splicing, adaptive thresholding, and contour approximation to crop the cell-lines CNN Model Performance Confusion Matrix The recognition performance of the CNN model on the augmented dataset\nFinal Cell Counts for the Input Micrograph The final result in form of cell-counts for the ROI (region of interest) window selected in the DIH micrograph ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"102ed8bd008cc5f35bfe45be63dd2036","permalink":"https://rvaghashiya.github.io/post/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/index1/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/index1/","section":"post","summary":"Intelligent Digital Inline Holographic Micrograph (DIHM) Cell-Enhancement and Characterization Project Intro Developing a novel and efficient neural model for detecting and classifying cells such as RBC, WBC, and cancer cells, etc.","tags":["Academic","Research","Deep Learning","Pathology"],"title":"Intelligent Digital Inline Holographic Micrograph (DIHM) Cell-Enhancement and Characterization","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34;\rif porridge == \u0026#34;blueberry\u0026#34;:\rprint(\u0026#34;Eating...\u0026#34;)\rMath In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\rPress Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\rPress the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}}\r{{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}\rCustom CSS Example Letâ€™s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\rQuestions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://rvaghashiya.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Rajkumar Vaghashiya","Dr. Nishant Doshi (Assistant Professor, PDPU)"],"categories":["Demo"],"content":"Project Goals IoT project to simulate a smart university for resource usage optimization, funded by ORSP-PDPU. Annual financial grant of â‚¹ 1,45,000 sanctioned by the university for the project development Prototype modules installed in Computer Lab: Light Control, Lab Temperature Control, Authorized Personnel Access. Raspberry Pi and Arduino (prototyping), MQTT (communication), Firebase/MongoDB (database), Python Intro Every industry in the world is incorporating smart devices to enhance their service and productivityâ€” Healthcare via various wearable devices, Transport industry via the intelligent transport system, Agriculture industry via smart farming sensors and farm monitoring sensors.\nBut very few researches are focused on the use of internet of things for improving quality of university working functionality as well as education. University campuses are ideal place to impart smart environment, and would pique studentâ€™s interest in learning about the latest equipment and tech.\nIn this idea of a smart university, we will research on various modules for the smart university, available microcontrollers, communication protocols, and mobile applications as user interface. As an outcome of this project, we expect to design and implement robust architecture for the smart classroom.\nFor the current project setup, we have focused on the aspects of smart classroom such as:\nSmart Classroom Smart Energy Controller Smart Security Following this, we will subsequently expand to other modules of a smart university.\nAim With an objective to implement secure smart university concept in PDPU campus, in a modular manner, following goals were achieved:\nPDPU becomes the first university of Gujarat to take step towards SMART University. Electricity consumption for the experimented room was found to reduce by a significant amount (20% percent electricity savings). The system permits constant monitoring of smoke and temperature which can help in an early identification of disaster. Modules Implemented during the project Virtual setup of fully automatic smart lights for labs Different classes or labs have replication of same setup connected to a central MySQL Server. The setup includes IR Motion sensor and Thermal sensor to detect any motion in the lab and to check whether any person is there in the lab. Accordingly the lights connected to microcontroller via relay are turned on and off. All the data is recorded to SQL server which is used further for additional analytics.\nVirtual setup of partially automatic smart labs It shows smart AC, light and fan for any class or labs which are controlled and monitored by an android mobile application. Mobile application have authentication which ensures only trusted users/owners have control over it. All the commands sent by the mobile apps and the responses to it are stored in the SQL database.\nReport Energy Aware IoT based Automated Smart Lighting and CCTV System using MQTT and MySQL\nIoT â€“ Principles and Paradigms\n","date":1547510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"89ee68c2e0f5257330ec00bf037ee849","permalink":"https://rvaghashiya.github.io/post/secure-and-smart-university/index1/","publishdate":"2019-01-15T00:00:00Z","relpermalink":"/post/secure-and-smart-university/index1/","section":"post","summary":"Project Goals IoT project to simulate a smart university for resource usage optimization, funded by ORSP-PDPU. Annual financial grant of â‚¹ 1,45,000 sanctioned by the university for the project development Prototype modules installed in Computer Lab: Light Control, Lab Temperature Control, Authorized Personnel Access.","tags":["Academic","Research","IoT","Smart University","Applications"],"title":"Secure \u0026 Smart University","type":"post"},{"authors":["Rajkumar Vaghashiya","Dr. Nishant Doshi (Assistant Professor, PDPU)"],"categories":null,"content":"Project Goals IoT project to simulate a smart university for resource usage optimization, funded by ORSP-PDPU. Annual financial grant of â‚¹ 1,45,000 sanctioned by the university for the project development Prototype modules installed in Computer Lab: Light Control, Lab Temperature Control, Authorized Personnel Access. Raspberry Pi and Arduino, MQTT , Firebase/MongoDB , Python Intro Every industry in the world is incorporating smart devices to enhance their service and productivityâ€” Healthcare via various wearable devices, Transport industry via the intelligent transport system, Agriculture industry via smart farming sensors and farm monitoring sensors.\nBut very few researches are focused on the use of internet of things for improving quality of university working functionality as well as education. University campuses are ideal place to impart smart environment, and would pique studentâ€™s interest in learning about the latest equipment and tech.\nIn this idea of a smart university, we will research on various modules for the smart university, available microcontrollers, communication protocols, and mobile applications as user interface. As an outcome of this project, we expect to design and implement robust architecture for the smart classroom.\nFor the current project setup, we have focused on the aspects of smart classroom such as:\nSmart Classroom Smart Energy Controller Smart Security Following this, we will subsequently expand to other modules of a smart university.\nAim With an objective to implement secure smart university concept in PDPU campus, in a modular manner, following goals were achieved:\nPDPU becomes the first university of Gujarat to take step towards SMART University. Electricity consumption for the experimented room was found to reduce by a significant amount (20% percent electricity savings). The system permits constant monitoring of smoke and temperature which can help in an early identification of disaster. Modules Implemented during the project Virtual setup of fully automatic smart lights for labs Different classes or labs have replication of same setup connected to a central MySQL Server. The setup includes IR Motion sensor and Thermal sensor to detect any motion in the lab and to check whether any person is there in the lab. Accordingly the lights connected to microcontroller via relay are turned on and off. All the data is recorded to SQL server which is used further for additional analytics.\nVirtual setup of partially automatic smart labs It shows smart AC, light and fan for any class or labs which are controlled and monitored by an android mobile application. Mobile application have authentication which ensures only trusted users/owners have control over it. All the commands sent by the mobile apps and the responses to it are stored in the SQL database.\nReport Energy Aware IoT based Automated Smart Lighting and CCTV System using MQTT and MySQL\nIoT â€“ Principles and Paradigms\n","date":1547510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510348980,"objectID":"f6c7d47e9a6eccdba33b90ba8d2ae90e","permalink":"https://rvaghashiya.github.io/project/secure-and-smart-university/","publishdate":"2019-01-15T00:00:00Z","relpermalink":"/project/secure-and-smart-university/","section":"project","summary":"IoT project to simulate a smart university for resource usage optimization with prototype deployed modules installed in Computer Lab.","tags":["Internet of Things","Research","Applications","Smart University","Academic","IoT"],"title":"Secure and Smart University","type":"project"},{"authors":["Rajkumar Vaghashiya","Dr. Mohendra Roy (Assistant Professor, PDPU)"],"categories":null,"content":"Project Intro Developing a novel and efficient neural model for detecting and classifying cells such as RBC, WBC, and cancer cells, etc. (cellular pathology) from DIH (Digital Inline Holographic) microscopy images\nImplementation on a resource-constrained device to develop a cheap, reliable and portable point-of-care testing facility for diagnosis of pathological diseases, especially for usage in rural areas\nHighlights Segment cell-lines in DIH micrograph; performs signal enhancement using CNN-based autoencoder, followed by the cell-line characterization ROC-AUC: \u0026gt;0.98 for RBC, WBC, and microbeads; \u0026gt;0.88 for cancer cells HepG2 and MCF7 Easy accommodation of newer cell-lines. Python, TensorFlow, OpenCV Preliminary work published at the 8th IEEE International Conference on Healthcare Informatics (ICHI) 2020 Breast Cancer Disease Statistics Limitations of Traditional Optic Microscopy Aim The aim of the research is to create a\nCheap, Reliable, Adaptable, Portable, and Intelligent real-time point-of-care testing facility that could be used in resource-constrained environments, especially such as those in a rural setting. Dataset Cell-lines used in the project Current Progress Cell-line EDA The colored cell-lines is the result of EDA on the statistical properties and pixel intensity in the images\nSegmentation Results The segmentation involves bit-plane splicing, adaptive thresholding, and contour approximation to crop the cell-lines CNN Model Performance Confusion Matrix The recognition performance of the CNN model on the augmented dataset\nFinal Cell Counts for the Input Micrograph The final result in form of cell-counts for the ROI (region of interest) window selected in the DIH micrograph ","date":1546896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"926e7a4784fde19192848da14b57444a","permalink":"https://rvaghashiya.github.io/project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/","publishdate":"2019-01-07T21:20:00Z","relpermalink":"/project/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/","section":"project","summary":"Cell-line segmentation from DIH micrograph. Signal enhancement using CNN-based Autoencoder. Cell-line characterization using CNNs. Easy accommodation of newer cell-lines using transfer learning.","tags":["Deep Learning","DIH imaging","Pathology","Image Segmentation","Image Denoising","Transfer Learning","Research","Academic","Pathology"],"title":"Intelligent Digital Inline Holographic Micrograph (DIHM) Cell-Enhancement and Characterization","type":"project"},{"authors":["Rajkumar Vaghashiya","Riya Thakore","Chintan Patel","Nishant Doshi"],"categories":null,"content":"","date":1546375089,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546375089,"objectID":"52e6399286dc42569fee444a6fb3f2df","permalink":"https://rvaghashiya.github.io/publication/iot--principles-and-paradigms/","publishdate":"2019-01-01T20:38:09.316Z","relpermalink":"/publication/iot--principles-and-paradigms/","section":"publication","summary":"","tags":["Internet of Things (IoT)","Applications"],"title":"IoT â€“ principles and paradigms","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://rvaghashiya.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]