<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Demo | Rajkumar Vaghashiya</title>
    <link>https://rvaghashiya.github.io/category/demo/</link>
      <atom:link href="https://rvaghashiya.github.io/category/demo/index.xml" rel="self" type="application/rss+xml" />
    <description>Demo</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2022 Rajkumar Vaghashiya</copyright><lastBuildDate>Tue, 01 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://rvaghashiya.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Demo</title>
      <link>https://rvaghashiya.github.io/category/demo/</link>
    </image>
    
    <item>
      <title>Smart, Portable, and Cost-effective ELISA Reader</title>
      <link>https://rvaghashiya.github.io/post/smart-portable-and-cost-effective-elisa-reader/index1/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://rvaghashiya.github.io/post/smart-portable-and-cost-effective-elisa-reader/index1/</guid>
      <description>&lt;h2 id=&#34;project-goals&#34;&gt;Project Goals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cost-effective and portable ELISA reader with AI-aided diagnosis for adaptive calibration&lt;/li&gt;
&lt;li&gt;Image processing based qualitative and quantitative real-time analysis for improved sensitivity and specificity&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;current-progress&#34;&gt;Current Progress&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Completed precise micro-plate well localization&lt;/li&gt;
&lt;li&gt;Well localization utilizes bit plane slicing, masking and contouring, and shape appropriation&lt;/li&gt;
&lt;li&gt;Extracts basic color information, measuring light intensity in RBG colorspace, for each well&lt;/li&gt;
&lt;li&gt;Provides normalized measure of intenstity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;note&#34;&gt;Note&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Uses standard well-reference: A-H for rows, 1 to 12 for columns&lt;/li&gt;
&lt;li&gt;‘00’ for base/reference well&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sample: Segmented Microplate-wells (Pic shows a zoomed section of the microplate)














&lt;figure  id=&#34;figure-sample-segmented-microplate-wells-pic-shows-a-zoomed-section-of-the-microplate&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;contoured_img.png&#34; alt=&#34;Sample: Segmented Microplate-wells (Pic shows a zoomed section of the microplate)&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Sample: Segmented Microplate-wells (Pic shows a zoomed section of the microplate)
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Locations of Segmented Wells














&lt;figure  id=&#34;figure-locations-of-segmented-wells&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;well_data.png&#34; alt=&#34;Locations of Segmented Wells&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Locations of Segmented Wells
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;













&lt;figure  id=&#34;figure-a-cropped-well-filled-with-a-solution&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;45well.png&#34; alt=&#34;A cropped Well filled with a solution&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      A cropped Well filled with a solution
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;td&gt;













&lt;figure  id=&#34;figure-a-cropped-empty-well&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;56well.png&#34; alt=&#34;A cropped empty Well&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      A cropped empty Well
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Well boundary colored has been colored for reference in the above figures.&lt;/p&gt;
&lt;h3 id=&#34;note-1&#34;&gt;Note:&lt;/h3&gt;
&lt;p&gt;Full work till date cannot be disclosed&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DIH-Cell-Augmentor</title>
      <link>https://rvaghashiya.github.io/project/dih-cell-augmentor/</link>
      <pubDate>Sat, 01 Aug 2020 23:09:00 +0000</pubDate>
      <guid>https://rvaghashiya.github.io/project/dih-cell-augmentor/</guid>
      <description>&lt;h1 id=&#34;dih-cell-augmentor&#34;&gt;DIH-Cell-Augmentor&lt;/h1&gt;
&lt;p&gt;A setup for augmentation of biological cells captured via Digital Inline Holography&lt;/p&gt;
&lt;h2 id=&#34;aim&#34;&gt;Aim&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A script to augment the cell-lines obtained via DIHM.&lt;/li&gt;
&lt;li&gt;It generates rotated copies of the input cell sample, while maintaining the same image dimensions and near-similar sample background, using OpenCV.&lt;/li&gt;
&lt;li&gt;The augmented samples preserve the spatial features as well as the statistical distribution present in the input sample.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Load dataset, having images grouped under labels&lt;/li&gt;
&lt;li&gt;Augment cell sample by rotation at specific angle&lt;/li&gt;
&lt;li&gt;Store augmented images in a csv&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;augmentation-procedure&#34;&gt;Augmentation Procedure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Create a mask to extract the cell background&lt;/li&gt;
&lt;li&gt;Negate the above mask to extract the cell&lt;/li&gt;
&lt;li&gt;Rotate the image by specified degree, achieved via &lt;code&gt;getRotationMatrix2D&lt;/code&gt; and &lt;code&gt;warpAffine&lt;/code&gt; functions in cv2&lt;/li&gt;
&lt;li&gt;Extract the rotated cell signature using the negated mask&lt;/li&gt;
&lt;li&gt;Mask it with the extracted cell background to create an augmented image, with same shape as original image&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: The code is applicable only for augmentation of objects/cells lying within the maximum fitting circle which can be fit into the image&lt;/p&gt;
&lt;h2 id=&#34;pre-requisites&#34;&gt;Pre-requisites:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cv2&lt;/li&gt;
&lt;li&gt;glob, or alternatively use os&lt;/li&gt;
&lt;li&gt;numpy&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;demo&#34;&gt;Demo:&lt;/h2&gt;
&lt;h3 id=&#34;input-image&#34;&gt;Input Image:&lt;/h3&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;h3 id=&#34;augmented-images&#34;&gt;Augmented Images:&lt;/h3&gt;
&lt;p&gt;Augmented Images on rotation by 0, 10, 20, 30,&amp;hellip;..,350 degrees :&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;!-- raw HTML omitted --&gt;&lt;/th&gt;
&lt;th&gt;&lt;!-- raw HTML omitted --&gt;&lt;/th&gt;
&lt;th&gt;&lt;!-- raw HTML omitted --&gt;&lt;/th&gt;
&lt;th&gt;&lt;!-- raw HTML omitted --&gt;&lt;/th&gt;
&lt;th&gt;&lt;!-- raw HTML omitted --&gt;&lt;/th&gt;
&lt;th&gt;&lt;!-- raw HTML omitted --&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;!-- raw HTML omitted --&gt;&lt;/td&gt;
&lt;td&gt;&lt;!-- raw HTML omitted --&gt;&lt;/td&gt;
&lt;td&gt;&lt;!-- raw HTML omitted --&gt;&lt;/td&gt;
&lt;td&gt;&lt;!-- raw HTML omitted --&gt;&lt;/td&gt;
&lt;td&gt;&lt;!-- raw HTML omitted --&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;!-- raw HTML omitted --&gt;&lt;/td&gt;
&lt;td&gt;&lt;!-- raw HTML omitted --&gt;&lt;/td&gt;
&lt;td&gt;&lt;!-- raw HTML omitted --&gt;&lt;/td&gt;
&lt;td&gt;&lt;!-- raw HTML omitted --&gt;&lt;/td&gt;
&lt;td&gt;&lt;!-- raw HTML omitted --&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note: Black dot has been added for easier vizualization&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/raj-98/DIH-Cell-Augmentor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Git: DIH Cell Augmentor&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computer Pointer Controller</title>
      <link>https://rvaghashiya.github.io/project/computer-pointer-controller/</link>
      <pubDate>Thu, 09 Jul 2020 23:07:00 +0000</pubDate>
      <guid>https://rvaghashiya.github.io/project/computer-pointer-controller/</guid>
      <description>&lt;h1 id=&#34;computer-pointer-controller&#34;&gt;Computer Pointer Controller&lt;/h1&gt;
&lt;p&gt;Computer Pointer Controller is a smart application which leverages pre-trained OpenVino models wherein it infers the gaze direction of the subject in a video or webcam feed, and moves the mouse pointer accordingly in that direction.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;aim&#34;&gt;Aim&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To develop a smart application that infers the gaze direction of the subject in a video or webcam feed, and moves the mouse pointer accordingly in that direction&lt;/li&gt;
&lt;li&gt;Leverage image processing and pre-trained OpenVINO-models based inference to deliver real-time results&lt;/li&gt;
&lt;li&gt;Analyze the impact of model precision, type of input video streaming, and the effect of various edge cases and their effect on the results&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-set-up-and-installation&#34;&gt;Project Set Up and Installation&lt;/h2&gt;
&lt;h3 id=&#34;setup-details&#34;&gt;Setup Details&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Details&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Programming Language&lt;/td&gt;
&lt;td&gt;Python 3.&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenVino Toolkit Version&lt;/td&gt;
&lt;td&gt;2020.&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hardware Used&lt;/td&gt;
&lt;td&gt;Intel CPU i7 3rd Gen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Enviroments Used&lt;/td&gt;
&lt;td&gt;Windows WSL, Ubuntu 18.04&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;OpenVino installation guide can be found &lt;a href=&#34;https://docs.openvinotoolkit.org/latest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;directory-structure&#34;&gt;Directory Structure&lt;/h3&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h3 id=&#34;application-workflow-pipeline&#34;&gt;Application Workflow Pipeline&lt;/h3&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;benchmarks&#34;&gt;Benchmarks&lt;/h2&gt;
&lt;h3 id=&#34;performance-evaluation-on-cpu&#34;&gt;Performance Evaluation on CPU&lt;/h3&gt;
&lt;p&gt;Total frames analyzed: 59&lt;/p&gt;
&lt;p&gt;Face Detection Model takes nearly 287 ms of load time and 38 ms of total inference time.&lt;/p&gt;
&lt;p&gt;Model Load Time&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;FP16&lt;/th&gt;
&lt;th&gt;FP32&lt;/th&gt;
&lt;th&gt;FP16-INT8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Facial Landmarks Detection&lt;/td&gt;
&lt;td&gt;122.802 ms&lt;/td&gt;
&lt;td&gt;205.130 ms&lt;/td&gt;
&lt;td&gt;558.811 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Headpose Estimation&lt;/td&gt;
&lt;td&gt;187.159 ms&lt;/td&gt;
&lt;td&gt;340.448 ms&lt;/td&gt;
&lt;td&gt;375.702 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gaze Estimation&lt;/td&gt;
&lt;td&gt;206.344 ms&lt;/td&gt;
&lt;td&gt;361.897 ms&lt;/td&gt;
&lt;td&gt;455.502 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Total Load Time&lt;/td&gt;
&lt;td&gt;0.82 s&lt;/td&gt;
&lt;td&gt;1.188 s&lt;/td&gt;
&lt;td&gt;1.67 s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Model Inference Time&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;FP16&lt;/th&gt;
&lt;th&gt;FP32&lt;/th&gt;
&lt;th&gt;FP16-INT8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Facial Landmarks Detection&lt;/td&gt;
&lt;td&gt;1.0 ms&lt;/td&gt;
&lt;td&gt;1.0 ms&lt;/td&gt;
&lt;td&gt;1.1 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Headpose Estimation&lt;/td&gt;
&lt;td&gt;2.7 ms&lt;/td&gt;
&lt;td&gt;2.7 ms&lt;/td&gt;
&lt;td&gt;2.6 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gaze Estimation&lt;/td&gt;
&lt;td&gt;3.4 ms&lt;/td&gt;
&lt;td&gt;3.3 ms&lt;/td&gt;
&lt;td&gt;2.5 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Total Inference Time&lt;/td&gt;
&lt;td&gt;11.1 s&lt;/td&gt;
&lt;td&gt;11.4 s&lt;/td&gt;
&lt;td&gt;11.7 s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FPS&lt;/td&gt;
&lt;td&gt;5.315&lt;/td&gt;
&lt;td&gt;5.363&lt;/td&gt;
&lt;td&gt;5.042&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The inference times with respect to the models depict the average inference time per frame.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The results have been benchmarked on async inference&lt;/li&gt;
&lt;li&gt;Model Load time for FP16 is the least, followed by FP32, followed by INT8&lt;/li&gt;
&lt;li&gt;Average inference time is greatest for INT8, followed by FP32, followed by FP16, which may be attributed to the trade-off between greater information involved in the computation of models having relatively higher precision and the hardware architecture of the system under use.&lt;/li&gt;
&lt;li&gt;A decrease in the model precision leads to loss of the model accuracy at the cost of increasing the inference speed.&lt;/li&gt;
&lt;li&gt;Frames are processed relatively faster in FP32, followed by FP16 and then INT8.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;edge-cases&#34;&gt;Edge Cases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In case a person is not detected in the frame, the incident is logged and processing moves on to the next frame in the queue.&lt;/li&gt;
&lt;li&gt;In case of multiple persons in the frame, only the face of the first person to be detected will be used in the further processing.&lt;/li&gt;
&lt;li&gt;In case the new mouse corrdinates from gaze estimation cannot be accomodated on the current screen resolution, the event is logged and processing moves onto next frame in the sequence.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/raj-98/Computer-Pointer-Controller&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Git: Computer Pointer Controller&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>People Counter App</title>
      <link>https://rvaghashiya.github.io/project/people-counter-app/</link>
      <pubDate>Thu, 25 Jun 2020 22:50:00 +0000</pubDate>
      <guid>https://rvaghashiya.github.io/project/people-counter-app/</guid>
      <description>&lt;h1 id=&#34;deploy-a-people-counter-app-at-the-edge&#34;&gt;Deploy a People Counter App at the Edge&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Details&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Programming Language:&lt;/td&gt;
&lt;td&gt;Python 3.5 or 3.6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./people-counter-image.png&#34; alt=&#34;people-counter-python&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;what-it-does&#34;&gt;What it Does&lt;/h2&gt;
&lt;p&gt;The people counter application will demonstrate how to create a smart video IoT solution using Intel® hardware and software tools. The app will detect people in a designated area, providing the number of people in the frame, average duration of people in frame, and total count.&lt;/p&gt;
&lt;h2 id=&#34;aim&#34;&gt;Aim&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Create an app that will help maintain social distancing at public places likes grocery stores and shopping malls, to curb the spread of Covid-19 virus&lt;/li&gt;
&lt;li&gt;The app will detect people in a designated area, providing the number of people in the frame, average duration of people in frame, and total count.&lt;/li&gt;
&lt;li&gt;The people counter application is a smart video IoT solution based on Intel® hardware and software tools.&lt;/li&gt;
&lt;li&gt;Utilizes deep learning, image processing, and OpenVINO-based inferencing at the edge.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-it-works&#34;&gt;How it Works&lt;/h2&gt;
&lt;p&gt;The counter will use the Inference Engine included in the Intel® Distribution of OpenVINO™ Toolkit. The model used should be able to identify people in a video frame. The app should count the number of people in the current frame, the duration that a person is in the frame (time elapsed between entering and exiting a frame) and the total count of people. It then sends the data to a local web server using the Paho MQTT Python package.&lt;/p&gt;
&lt;p&gt;You will choose a model to use and convert it with the Model Optimizer.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./arch_diagram.png&#34; alt=&#34;architectural diagram&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;
&lt;h3 id=&#34;hardware&#34;&gt;Hardware&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;6th to 10th generation Intel® Core™ processor with Iris® Pro graphics or Intel® HD Graphics.&lt;/li&gt;
&lt;li&gt;OR use of Intel® Neural Compute Stick 2 (NCS2)&lt;/li&gt;
&lt;li&gt;OR Udacity classroom workspace for the related course&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;software&#34;&gt;Software&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Intel® Distribution of OpenVINO™ toolkit 2019 R3 release&lt;/li&gt;
&lt;li&gt;Node v6.17.1&lt;/li&gt;
&lt;li&gt;Npm v3.10.10&lt;/li&gt;
&lt;li&gt;CMake&lt;/li&gt;
&lt;li&gt;MQTT Mosca server&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/raj-98/People-Counter-App&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Git: People Counter App&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/udacity/nd131-openvino-fundamentals-project-starter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reference Repo&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intelligent Digital Inline Holographic Micrograph (DIHM) Cell-Enhancement and Characterization</title>
      <link>https://rvaghashiya.github.io/post/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/index1/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://rvaghashiya.github.io/post/intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization/index1/</guid>
      <description>&lt;h2 id=&#34;intelligent-digital-inline-holographic-micrograph-dihm-cell-enhancement-and-characterization&#34;&gt;Intelligent Digital Inline Holographic Micrograph (DIHM) Cell-Enhancement and Characterization&lt;/h2&gt;
&lt;h2 id=&#34;project-intro&#34;&gt;Project Intro&lt;/h2&gt;
&lt;p&gt;Developing a novel and efficient neural model for detecting and classifying cells such as RBC, WBC, and cancer cells, etc.(cellular pathology) from DIH(Digital Inline Holographic) microscopy images&lt;/p&gt;
&lt;p&gt;Implementation on a resource-constrained device to develop a cheap, reliable and portable point-of-care testing facility for diagnosis of pathological diseases, especially for usage in rural areas&lt;/p&gt;
&lt;h2 id=&#34;highlights&#34;&gt;Highlights&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Segment cell-lines in DIH micrograph; performs signal enhancement using CNN-based autoencoder, followed by the cell-line characterization&lt;/li&gt;
&lt;li&gt;ROC-AUC: &amp;gt;0.98 for RBC, WBC, and microbeads; &amp;gt;0.88 for cancer cells HepG2 and MCF7&lt;/li&gt;
&lt;li&gt;Easy accommodation of newer cell-lines. Python, TensorFlow, OpenCV&lt;/li&gt;
&lt;li&gt;Preliminary work &lt;a href=&#34;https://ieeexplore.ieee.org/document/9374330&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;published&lt;/a&gt; at the 8th IEEE ICHI&lt;/li&gt;
&lt;/ul&gt;














&lt;figure  id=&#34;figure-breast-cancer-disease-statistics&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;img1.png&#34; alt=&#34;Breast Cancer Disease Statistics&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Breast Cancer Disease Statistics
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-limitations-of-traditional-optic-microscopy&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;img2.png&#34; alt=&#34;Limitations of Traditional Optic Microscopy&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Limitations of Traditional Optic Microscopy
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;aim&#34;&gt;Aim&lt;/h2&gt;
&lt;p&gt;The aim of the research is to create a&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cheap,&lt;/li&gt;
&lt;li&gt;Reliable,&lt;/li&gt;
&lt;li&gt;Adaptable,&lt;/li&gt;
&lt;li&gt;Portable, and&lt;/li&gt;
&lt;li&gt;Intelligent
real-time point-of-care testing facility that could be used in resource-constrained environments, especially such as those in a rural setting.&lt;/li&gt;
&lt;/ul&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;img3.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;














&lt;figure  id=&#34;figure-cell-lines-used-in-the-project&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;cells.png&#34; alt=&#34;Cell-lines used in the project&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Cell-lines used in the project
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;current-progress&#34;&gt;Current Progress&lt;/h2&gt;














&lt;figure  id=&#34;figure-cell-line-eda&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;img4.png&#34; alt=&#34;Cell-line EDA&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Cell-line EDA
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The colored cell-lines is the result of EDA on the statistical properties and pixel intensity in the images&lt;/p&gt;
&lt;h2 id=&#34;segmentation-results&#34;&gt;Segmentation Results&lt;/h2&gt;














&lt;figure  id=&#34;figure-the-segmentation-involves-bit-plane-splicing-adaptive-thresholding-and-contour-approximation-to-crop-the-cell-lines&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;segmented_cells.png&#34; alt=&#34;The segmentation involves bit-plane splicing, adaptive thresholding, and contour approximation to crop the cell-lines&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The segmentation involves bit-plane splicing, adaptive thresholding, and contour approximation to crop the cell-lines
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;cnn-model-performance&#34;&gt;CNN Model Performance&lt;/h2&gt;














&lt;figure  id=&#34;figure-confusion-matrix&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;confmatrix_with_acc_prevv.png&#34; alt=&#34;Confusion Matrix&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Confusion Matrix
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The recognition performance of the CNN model on the augmented dataset&lt;/p&gt;
&lt;h2 id=&#34;final-cell-counts-for-the-input-micrograph&#34;&gt;Final Cell Counts for the Input Micrograph&lt;/h2&gt;














&lt;figure  id=&#34;figure-the-final-result-in-form-of-cell-counts-for-the-roi-region-of-interest-window-selected-in-the-dih-micrograph&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;final_count.png&#34; alt=&#34;The final result in form of cell-counts for the ROI (region of interest) window selected in the DIH micrograph&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The final result in form of cell-counts for the ROI (region of interest) window selected in the DIH micrograph
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Secure &amp; Smart University</title>
      <link>https://rvaghashiya.github.io/post/secure-and-smart-university/index1/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://rvaghashiya.github.io/post/secure-and-smart-university/index1/</guid>
      <description>&lt;h2 id=&#34;project-goals&#34;&gt;Project Goals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;IoT project to simulate a smart university for resource usage optimization, funded by ORSP-PDPU.&lt;/li&gt;
&lt;li&gt;Annual financial grant of ₹ 1,45,000 sanctioned by the university for the project development&lt;/li&gt;
&lt;li&gt;Prototype modules installed in Computer Lab: Light Control, Lab Temperature Control, Authorized Personnel Access.&lt;/li&gt;
&lt;li&gt;Raspberry Pi and Arduino (prototyping), MQTT (communication), Firebase/MongoDB (database), Python&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;Every industry in the world is incorporating smart devices to enhance their service and productivity— Healthcare via various wearable devices, Transport industry via the intelligent transport system, Agriculture industry via smart farming sensors and farm monitoring sensors.&lt;/p&gt;
&lt;p&gt;But very few researches are focused on the use of internet of things for improving quality of university working functionality as well as education. University campuses are ideal place to impart smart environment, and would pique student&amp;rsquo;s interest in learning about the latest equipment and tech.&lt;/p&gt;
&lt;p&gt;In this idea of a smart university, we will research on various modules for the smart university, available microcontrollers, communication protocols, and mobile applications as user interface. As an outcome of this project, we expect to design and implement robust architecture for the smart classroom.&lt;/p&gt;
&lt;p&gt;For the current project setup, we have focused on the aspects of smart classroom such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Smart Classroom&lt;/li&gt;
&lt;li&gt;Smart Energy Controller&lt;/li&gt;
&lt;li&gt;Smart Security&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Following this, we will subsequently expand to other modules of a smart university.&lt;/p&gt;
&lt;h2 id=&#34;aim&#34;&gt;Aim&lt;/h2&gt;
&lt;p&gt;With an objective to implement secure smart university concept in PDPU campus, in a modular manner, following goals were achieved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PDPU becomes the first university of Gujarat to take step towards SMART University.&lt;/li&gt;
&lt;li&gt;Electricity consumption for the experimented room was found to reduce by a significant amount (20% percent electricity savings).&lt;/li&gt;
&lt;li&gt;The system permits constant monitoring of smoke and temperature which can help in an early identification of disaster.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;modules-implemented-during-the-project&#34;&gt;Modules Implemented during the project&lt;/h2&gt;














&lt;figure  id=&#34;figure-virtual-setup-of-fully-automatic-smart-lights-for-labs&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;Module1.png&#34; alt=&#34;Virtual setup of fully automatic smart lights for labs&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Virtual setup of fully automatic smart lights for labs
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Different classes or labs have replication of same setup connected to a central MySQL Server. The setup includes IR Motion sensor and Thermal sensor to detect any motion in the lab and to check whether any person is there in the lab. Accordingly the lights connected to microcontroller via relay are turned on and off. All the data is recorded to SQL server which is used further for additional analytics.&lt;/p&gt;














&lt;figure  id=&#34;figure-virtual-setup-of-partially-automatic-smart-labs&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;Module2.png&#34; alt=&#34;Virtual setup of partially automatic smart labs&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Virtual setup of partially automatic smart labs
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;It shows smart AC, light and fan for any class or labs which are controlled and monitored by an android mobile application. Mobile application have authentication which ensures only trusted users/owners have control over it. All the commands sent by the mobile apps and the responses to it are stored in the SQL database.&lt;/p&gt;
&lt;h2 id=&#34;report&#34;&gt;Report&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1eYKJPjkcWY9G9gl6tPw9Nrq2l8Xxx5Mw/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Energy Aware IoT based Automated Smart Lighting and CCTV System using MQTT and MySQL&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.warse.org/IJATCSE/static/pdf/file/ijatcse24816sl2019.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IoT – Principles and Paradigms&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
